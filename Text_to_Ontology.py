# -*- coding: utf-8 -*-
"""Text to Ontology.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q6Ry-XKkeqmYWqsMFyrqRGeuWr07u2ky
"""

!pip3 install transformers owlready2 docx2txt fasttext python-docx

"""# Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import torch
import docx
from transformers import BertTokenizer, BertModel, AutoModel
from gensim.models.fasttext import FastText
import matplotlib.pyplot as plt
import string
import re
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from owlready2 import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import docx2txt
import fasttext
from transformers import logging
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cdist
from scipy.spatial.distance import minkowski
from scipy.spatial.distance import cityblock
import hashlib
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score

logging.set_verbosity_error()
# %matplotlib inline

nltk.download("punkt")
nltk.download('stopwords')
nltk.download("wordnet")
nltk.download("omw-1.4")

import warnings

# Ignore all warnings
warnings.filterwarnings('ignore')

"""# Pre-processing"""

# Load the pre-trained spaCy model
nlp = spacy.load("en_core_web_sm")

# Load the input docx file
doc = docx.Document("Input Text.docx")

# Extract the text from the docx file
text = "\n".join([paragraph.text for paragraph in doc.paragraphs])

# Use the spaCy model to split the text into sentences
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]

# Filter valid sentences with atleast 2 nouns and a verb
valid_sentences = []
for sent in sentences:
    doc = nlp(sent)
    nouns = [token for token in doc if token.pos_ == "NOUN"]
    verbs = [token for token in doc if token.pos_ == "VERB"]
    if len(nouns) >= 2 and len(verbs) >= 1:
        # Filter out sentences with non-paired parenthesis
        if sent.count("(") == sent.count(")"):
            # Filter out sentences with ill-parsed formulas or composed terms
            if not re.search(r"\b\w+\([\w\s]+\)|\([\w\s]+\)\w+\b", sent):
                valid_sentences.append(sent)


valid_sentences

# Define a translation table to remove punctuations, symbols, and hyphens
translator = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences = [s.translate(translator).strip() for s in valid_sentences]

#Removing numbers if any
valid_sentences = [re.sub(r'\d+', '', x).strip() for x in valid_sentences]

#Removing unnecessary punctuations from the text
valid_sentences = [n.replace('\n', '') for n in valid_sentences]
valid_sentences = [n.replace('\t', '') for n in valid_sentences]
valid_sentences = [n.replace('–', '') for n in valid_sentences]

valid_sentences

len(valid_sentences)

"""# Ontologies"""

#Loading the Ontologies
mason = get_ontology("mason.owl").load()
saref4inma = get_ontology("saref4inma.rdf").load()
msdl = get_ontology("MSDL.owl").load()
pronto = get_ontology("bpo.rdf").load()
smo = get_ontology("smo.owl").load()

"""## Mason"""

# Get the list of classes with annotations
mason_class = []

for cls in mason.classes():
    annotations = cls.comment
    if annotations:
        mason_class.append(cls.name)

mason_class

mason.classes()
mason_cls = list(mason.classes())

mason.properties()
mason_property = list(mason.properties())

mason_annotations = []
for ann in mason_cls:
    print(f"\t{ann}: {ann.comment}")
    mason_annotations.append(str(ann.comment))

# Remove the full stop from each element
mason_annotations = [x.replace('.', '') for x in mason_annotations]

for i in range(len(mason_annotations)):
    mason_annotations[i] = mason_annotations[i] + "."

mason_annotations

mason_ann = ', '.join(mason_annotations)

# define a regular expression to match non-word characters except full stops
regex = re.compile('[%s]' % re.escape(string.punctuation.replace('.', '')))

# apply the regular expression to remove non-word characters
mason_ann = regex.sub('', mason_ann)
mason_ann = re.sub(r'\b(?<!\w)en\b(?!\w)', '', mason_ann)

mason_ann

sentences_mason = mason_ann.split('. ')
valid_sentences_mason = [s.strip() for s in sentences_mason]

# Define a translation table to remove punctuations, symbols, and hyphens
translator_mason = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences_mason = [s.translate(translator_mason).strip() for s in valid_sentences_mason]

#Removing unnecessary punctuations from the text
valid_sentences_mason = [n.replace('\n', '') for n in valid_sentences_mason]
valid_sentences_mason = [n.replace('locstr', '') for n in valid_sentences_mason]
valid_sentences_mason = [n.replace('-', '') for n in valid_sentences_mason]

valid_sentences_mason = [item for item in valid_sentences_mason if item != ' ' and item != '']

valid_sentences_mason

# Chexking for duplicates
unique_items = set(valid_sentences_mason)
duplicates = [item for item in unique_items if valid_sentences_mason.count(item) > 1]
duplicates

len(valid_sentences_mason)

"""### Bert Model"""

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Create empty lists for embeddings
bert_embeddings = []
bert_embeddings_mason = []

# Loop over sentences and calculate embeddings
for sent1 in valid_sentences:
    embedding1 = get_embedding(sent1, model, tokenizer)
    bert_embeddings.append(embedding1)
    for sent2 in valid_sentences_mason:
        embedding2 = get_embedding(sent2, model, tokenizer)
        bert_embeddings_mason.append(embedding2)

# Create dataframe
data_mason = {
    "Input_Sentence": [],
    "Annotation": [],
    "Input_Sentence_Embedding": [],
    "Annotation_Embedding": [],
}

for sent1, emb1 in zip(valid_sentences, bert_embeddings):
    for sent2, emb2 in zip(valid_sentences_mason, bert_embeddings_mason):
        data_mason["Input_Sentence"].append(sent1)
        data_mason["Annotation"].append(sent2)
        data_mason["Input_Sentence_Embedding"].append(emb1)
        data_mason["Annotation_Embedding"].append(emb2)

df_mason = pd.DataFrame(data_mason)

df_mason.head(10)

# Adding class to the dataframe
rep_class = []
for i in range(len(valid_sentences)):
    rep_class += mason_class

df_mason['Class'] = rep_class[:len(df_mason)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_mason['Cosine_Similarity'] = df_mason.apply(cosine_sim, axis=1)

df_mason.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_mason = df_mason.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_mason = pd.merge(df_mason, max_similarities_mason, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_mason.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_mason.reset_index(drop=True, inplace=True)

df_cosine_similarity_mason.head(10)

# Create a new dataframe for hash codes
df_cosine_similarity_hc_mason = df_cosine_similarity_mason.copy()

# Removing the Embedding columns
del df_cosine_similarity_hc_mason['Input_Sentence_Embedding']
del df_cosine_similarity_hc_mason['Annotation_Embedding']

# create a new column with the hash code of the Input Sentence column
df_cosine_similarity_hc_mason['Hash_Code'] = df_cosine_similarity_hc_mason['Input_Sentence'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())

df_cosine_similarity_hc_mason.head(3)

"""### Populating the Hash-Code within the ontology"""

# Iterate through each row of the dataframe
for index, col in df_cosine_similarity_hc_mason.iterrows():
    # Get the hash code and the class from the current row
    hash_code = col['Hash_Code']
    class_name = col['Class']
    # Iterate through the classes in the ontology
    for cls in class_name:
        # Get the namespace of the ontology
        ns = mason.get_namespace(base_iri=mason.base_iri)
        # Get the class from the namespace
        cls = getattr(ns, class_name, None)
        # Check if the class was found in the ontology
        if cls is not None and isinstance(cls, ThingClass):
            # Create a new instance of the class with the hash code as the name
            instance = cls(hash_code)
        else:
            print(f"Class {class_name} not found in ontology")
        break  # exit the inner loop since we've found the matching class

# Save the ontology
mason.save("mason.owl")

for i in mason.Entity.instances(): print(i)

# Print the individuals
for individual in mason.individuals():
    print(individual)

# Get a generator object for all individuals in the ontology
individuals = mason.individuals()

# Loop over the generator and print the URI of each individual
for individual in individuals:
    individual_uri = individual.iri
    print("Individual URI:", individual_uri)

# Iterate over all individuals in the ontology
for individual in mason.individuals():
    print("Individual:", individual.iri)
    # Get the list of classes that the individual is a direct instance of
    classes = individual.is_a
    # Print the classes
    for cls in classes:
        print("Class:", cls.iri)

"""### Fast-Text"""

# Convert the sentences to lowercase
lowercase_sentences = [sentence.lower() for sentence in valid_sentences]
lowercase_sentences_mason = [sentence.lower() for sentence in valid_sentences_mason]

# create a dictionary to map the original sentences with the converted lowercase sentences for further use
dict_mason = {}
dict_ann_mason = {}

# loop over the lists and add the elements as key-value pairs in the dictionary
for i in range(len(lowercase_sentences)):
    dict_mason[lowercase_sentences[i]] = valid_sentences[i]

for i in range(len(lowercase_sentences_mason)):
    dict_ann_mason[lowercase_sentences_mason[i]] = valid_sentences_mason[i]

# Split the sentences into words
lowercase_words = []
for sentence in lowercase_sentences:
    lowercase_words.extend(sentence.split())

lowercase_words_mason = []
for sentence in lowercase_sentences_mason:
    lowercase_words_mason.extend(sentence.split())

# Write the input sentences to a text file
with open('ft_sentences_mason.txt', 'w') as f:
    for s1 in lowercase_words:
        f.write(s1 + '\n')

# Write the annotations to a text file
with open('ft_sentences_ann_mason.txt', 'w') as f:
    for s2 in lowercase_words_mason:
        f.write(s2 + '\n')

# Create dataframe with every combination of sentences
combos = list(itertools.product(lowercase_sentences, lowercase_sentences_mason))
df_mason_ft = pd.DataFrame(combos, columns=["Input_Sentence", "Annotation"])

# Train the FastText model
ft_model_mason = fasttext.train_unsupervised('ft_sentences_mason.txt', model='skipgram', dim=100)
ft_model_ann_mason = fasttext.train_unsupervised('ft_sentences_ann_mason.txt', model='skipgram', dim=100)

# Function to create embeddings for the sentences
def create_embedding_mason(sentence):
    return ft_model_mason.get_sentence_vector(sentence)

def create_embedding_ann_mason(sentence):
    return ft_model_ann_mason.get_sentence_vector(sentence)

# Apply function to each row of the DataFrame
df_mason_ft['Input_Sentence_Embedding'] = df_mason_ft['Input_Sentence'].apply(create_embedding_mason)
df_mason_ft['Annotation_Embedding'] = df_mason_ft['Annotation'].apply(create_embedding_ann_mason)

df_mason_ft.head(10)

# Adding class to the dataframe
rep_class_ft = []
for i in range(len(lowercase_sentences)):
    rep_class_ft += mason_class

df_mason_ft['Class'] = rep_class_ft[:len(df_mason_ft)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_mason_ft['Cosine_Similarity'] = df_mason_ft.apply(cosine_sim, axis=1)

df_mason_ft.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_ft_mason = df_mason_ft.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_ft_mason = pd.merge(df_mason_ft, max_similarities_ft_mason, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_ft_mason.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_ft_mason.reset_index(drop=True, inplace=True)

df_cosine_similarity_ft_mason.head(20)

# Removing the Embedding columns
del df_cosine_similarity_ft_mason['Input_Sentence_Embedding']
del df_cosine_similarity_ft_mason['Annotation_Embedding']

"""### Comparing Bert vs Fast-Text Models"""

# Switching every row of the 'Input_Sentence' & 'Annotation' columns of the Fast-Text dataframe to the dictionary values for comparison
df_cosine_similarity_ft_mason['Input_Sentence'] = df_cosine_similarity_ft_mason['Input_Sentence'].map(dict_mason)
df_cosine_similarity_ft_mason['Annotation'] = df_cosine_similarity_ft_mason['Annotation'].map(dict_ann_mason)

# Creating new dataframe for comparison
df_compare_mason = pd.merge(df_cosine_similarity_hc_mason, df_cosine_similarity_ft_mason, how = 'left', on = 'Input_Sentence')

# Renaming the columns
df_compare_mason.rename(columns={'Annotation_x': 'Annotation_Bert', 'Annotation_y': 'Annotation_Fast-Text', 'Class_x': 'Class_Bert',
                                 'Class_y': 'Class_Fast-Text', 'Cosine_Similarity_x': 'Cosine_Similarity_Bert', 'Cosine_Similarity_y': 'Cosine_Similarity_Fast-Text'}, inplace=True)

# Removing the unused columns
del df_compare_mason['Hash_Code']

# Changing the positons of the columns
df_compare_mason = df_compare_mason.reindex(columns=['Input_Sentence', 'Annotation_Bert', 'Annotation_Fast-Text', 'Class_Bert', 'Class_Fast-Text', 'Cosine_Similarity_Bert', 'Cosine_Similarity_Fast-Text'])

df_compare_mason.head(15)

# Plotting the results
x_mason = df_compare_mason.index.values
y1_mason = df_compare_mason['Cosine_Similarity_Bert']
y2_mason = df_compare_mason['Cosine_Similarity_Fast-Text']

plt.plot(x_mason, y1_mason, label='Bert Similarity')
plt.plot(x_mason, y2_mason, label='Fast-Text Similarity')
plt.xlabel('Index')
plt.ylabel('Cosine Similarity')
plt.title('Comparison of Cosine Similarities between Bert & Fast-Text Models in Mason Ontology')
plt.legend()
plt.show()

"""## Saref4inma"""

# Check all the classes
for i in saref4inma.classes(): print(i)

# Creating a list with all the classes except 'core' & 'saref4bldg' classes
saref4inma_cls = list(saref4inma.classes())
saref4inma_cls = [elem for elem in saref4inma_cls if not str(elem).startswith('core') and not str(elem).startswith('saref4bldg')]

# Get the list of classes with annotations
saref4inma_class = []

for cls in saref4inma_cls:
    annotations = cls.comment
    if annotations:
        saref4inma_class.append(cls.name)

saref4inma_class

saref4inma.properties()
saref4inma_property = list(saref4inma.properties())

saref4inma_annotations = []
for ann in saref4inma_cls:
    print(f"\t{ann}: {ann.comment}")
    saref4inma_annotations.append(str(ann.comment))

# Remove the full stop from each element
saref4inma_annotations = [x.replace('.', '') for x in saref4inma_annotations]

for i in range(len(saref4inma_annotations)):
    saref4inma_annotations[i] = saref4inma_annotations[i] + "."

saref4inma_annotations

saref4inma_ann = ', '.join(saref4inma_annotations)

# define a regular expression to match non-word characters except full stops
regex = re.compile('[%s]' % re.escape(string.punctuation.replace('.', '')))

# apply the regular expression to remove non-word characters
saref4inma_ann = regex.sub('', saref4inma_ann)

saref4inma_ann

sentences_saref4inma = saref4inma_ann.split('. ')
valid_sentences_saref4inma = [s.strip() for s in sentences_saref4inma]

# Define a translation table to remove punctuations, symbols, and hyphens
translator_saref4inma = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences_saref4inma = [s.translate(translator_saref4inma).strip() for s in valid_sentences_saref4inma]

#Removing unnecessary punctuations from the text
valid_sentences_saref4inma = [n.replace('\n', '') for n in valid_sentences_saref4inma]
valid_sentences_saref4inma = [n.replace('\t', '') for n in valid_sentences_saref4inma]
valid_sentences_saref4inma = [n.replace('–', '') for n in valid_sentences_saref4inma]

valid_sentences_saref4inma = [item for item in valid_sentences_saref4inma if item != ' ' and item != '']

valid_sentences_saref4inma

# Chexking for duplicates
unique_items = set(valid_sentences_saref4inma)
duplicates = [item for item in unique_items if valid_sentences_saref4inma.count(item) > 1]
duplicates

len(valid_sentences_saref4inma)

"""### Bert Model"""

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Create empty lists for embeddings
bert_embeddings = []
bert_embeddings_saref4inma = []

# Loop over sentences and calculate embeddings
for sent1 in valid_sentences:
    embedding1 = get_embedding(sent1, model, tokenizer)
    bert_embeddings.append(embedding1)
    for sent2 in valid_sentences_saref4inma:
        embedding2 = get_embedding(sent2, model, tokenizer)
        bert_embeddings_saref4inma.append(embedding2)

# Create dataframe
data_saref4inma = {
    "Input_Sentence": [],
    "Annotation": [],
    "Input_Sentence_Embedding": [],
    "Annotation_Embedding": [],
}

for sent1, emb1 in zip(valid_sentences, bert_embeddings):
    for sent2, emb2 in zip(valid_sentences_saref4inma, bert_embeddings_saref4inma):
        data_saref4inma["Input_Sentence"].append(sent1)
        data_saref4inma["Annotation"].append(sent2)
        data_saref4inma["Input_Sentence_Embedding"].append(emb1)
        data_saref4inma["Annotation_Embedding"].append(emb2)

df_saref4inma = pd.DataFrame(data_saref4inma)

df_saref4inma.head(4)

# Adding class to the dataframe
rep_class = []
for i in range(len(valid_sentences)):
    rep_class += saref4inma_class

df_saref4inma['Class'] = rep_class[:len(df_saref4inma)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_saref4inma['Cosine_Similarity'] = df_saref4inma.apply(cosine_sim, axis=1)

df_saref4inma.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_saref4inma = df_saref4inma.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_saref4inma = pd.merge(df_saref4inma, max_similarities_saref4inma, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_saref4inma.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_saref4inma.reset_index(drop=True, inplace=True)

df_cosine_similarity_saref4inma.head(5)

# Create a new dataframe for hash codes
df_cosine_similarity_hc_saref4inma = df_cosine_similarity_saref4inma.copy()

# Removing the Embedding columns
del df_cosine_similarity_hc_saref4inma['Input_Sentence_Embedding']
del df_cosine_similarity_hc_saref4inma['Annotation_Embedding']

# create a new column with the hash code of the Input Sentence column
df_cosine_similarity_hc_saref4inma['Hash_Code'] = df_cosine_similarity_hc_saref4inma['Input_Sentence'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())

df_cosine_similarity_hc_saref4inma.head(3)

"""### Populating the Hash-Code within the ontology"""

# Iterate through each row of the dataframe
for index, row in df_cosine_similarity_hc_saref4inma.iterrows():
    # Get the hash code and the class from the current row
    hash_code = row['Hash_Code']
    class_name = row['Class']
    # Iterate through the classes in the ontology
    for cls in class_name:
        # Get the namespace of the ontology
        ns = saref4inma.get_namespace(base_iri=saref4inma.base_iri)
        # Get the class from the namespace
        cls = getattr(ns, class_name, None)
        # Check if the class was found in the ontology
        if cls is not None and isinstance(cls, ThingClass):
            # Create a new instance of the class with the hash code as the name
            instance = cls(hash_code)
        else:
            print(f"Class {class_name} not found in ontology")
        break  # exit the inner loop since we've found the matching class

# Save the ontology
saref4inma.save("saref4inma.rdf")

for i in saref4inma.ProductionEquipment.instances(): print(i)

# Print the individuals
for individual in saref4inma.individuals():
    print(individual)

# Get a generator object for all individuals in the ontology
individuals = saref4inma.individuals()

# Loop over the generator and print the URI of each individual
for individual in individuals:
    individual_uri = individual.iri
    print("Individual URI:", individual_uri)

# Iterate over all individuals in the ontology
for individual in saref4inma.individuals():
    print("Individual:", individual.iri)
    # Get the list of classes that the individual is a direct instance of
    classes = individual.is_a
    # Print the classes
    for cls in classes:
        print("Class:", cls.iri)

"""### Fast-Text"""

# Convert the sentences to lowercase
lowercase_sentences = [sentence.lower() for sentence in valid_sentences]
lowercase_sentences_saref4inma = [sentence.lower() for sentence in valid_sentences_saref4inma]

# create a dictionary to map the original sentences with the converted lowercase sentences for further use
dict_saref4inma = {}
dict_ann_saref4inma = {}

# loop over the lists and add the elements as key-value pairs in the dictionary
for i in range(len(lowercase_sentences)):
    dict_saref4inma[lowercase_sentences[i]] = valid_sentences[i]

for i in range(len(lowercase_sentences_saref4inma)):
    dict_ann_saref4inma[lowercase_sentences_saref4inma[i]] = valid_sentences_saref4inma[i]

# Split the sentences into words
lowercase_words = []
for sentence in lowercase_sentences:
    lowercase_words.extend(sentence.split())

lowercase_words_saref4inma = []
for sentence in lowercase_sentences_saref4inma:
    lowercase_words_saref4inma.extend(sentence.split())

# Write the input sentences to a text file
with open('ft_sentences_saref4inma.txt', 'w') as f:
    for s1 in lowercase_words:
        f.write(s1 + '\n')

# Write the annotations to a text file
with open('ft_sentences_ann_saref4inma.txt', 'w') as f:
    for s2 in lowercase_words_saref4inma:
        f.write(s2 + '\n')

# Create dataframe with every combination of sentences
combos = list(itertools.product(lowercase_sentences, lowercase_sentences_saref4inma))
df_saref4inma_ft = pd.DataFrame(combos, columns=["Input_Sentence", "Annotation"])

# Train the FastText model
ft_model_saref4inma = fasttext.train_unsupervised('ft_sentences_saref4inma.txt', model='skipgram', dim=100)
ft_model_ann_saref4inma = fasttext.train_unsupervised('ft_sentences_ann_saref4inma.txt', model='skipgram', dim=100)

# Function to create embeddings for the sentences
def create_embedding_saref4inma(sentence):
    return ft_model_saref4inma.get_sentence_vector(sentence)

def create_embedding_ann_saref4inma(sentence):
    return ft_model_ann_saref4inma.get_sentence_vector(sentence)

# Apply function to each row of the DataFrame
df_saref4inma_ft['Input_Sentence_Embedding'] = df_saref4inma_ft['Input_Sentence'].apply(create_embedding_saref4inma)
df_saref4inma_ft['Annotation_Embedding'] = df_saref4inma_ft['Annotation'].apply(create_embedding_ann_saref4inma)

df_saref4inma_ft.head(10)

# Adding class to the dataframe
rep_class_ft = []
for i in range(len(lowercase_sentences)):
    rep_class_ft += saref4inma_class

df_saref4inma_ft['Class'] = rep_class_ft[:len(df_saref4inma_ft)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_saref4inma_ft['Cosine_Similarity'] = df_saref4inma_ft.apply(cosine_sim, axis=1)

df_saref4inma_ft.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_ft_saref4inma = df_saref4inma_ft.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_ft_saref4inma = pd.merge(df_saref4inma_ft, max_similarities_ft_saref4inma, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_ft_saref4inma.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_ft_saref4inma.reset_index(drop=True, inplace=True)

df_cosine_similarity_ft_saref4inma.head(20)

# Removing the Embedding columns
del df_cosine_similarity_ft_saref4inma['Input_Sentence_Embedding']
del df_cosine_similarity_ft_saref4inma['Annotation_Embedding']

"""### Comparing Bert vs Fast-Text Models"""

# switch every row of the 'Input_Sentence' & 'Annotation' columns of the Fast-Text dataframe to the dictionary values for comparison
df_cosine_similarity_ft_saref4inma['Input_Sentence'] = df_cosine_similarity_ft_saref4inma['Input_Sentence'].map(dict_saref4inma)
df_cosine_similarity_ft_saref4inma['Annotation'] = df_cosine_similarity_ft_saref4inma['Annotation'].map(dict_ann_saref4inma)

# Creating new dataframe for comparison
df_compare_saref4inma = pd.merge(df_cosine_similarity_hc_saref4inma, df_cosine_similarity_ft_saref4inma, how = 'left', on = 'Input_Sentence')

# Renaming the columns
df_compare_saref4inma.rename(columns={'Annotation_x': 'Annotation_Bert', 'Annotation_y': 'Annotation_Fast-Text', 'Class_x': 'Class_Bert',
                                 'Class_y': 'Class_Fast-Text', 'Cosine_Similarity_x': 'Cosine_Similarity_Bert', 'Cosine_Similarity_y': 'Cosine_Similarity_Fast-Text'}, inplace=True)

# Removing unused columns
del df_compare_saref4inma['Hash_Code']

# Changing the positons of the columns
df_compare_saref4inma = df_compare_saref4inma.reindex(columns=['Input_Sentence', 'Annotation_Bert', 'Annotation_Fast-Text', 'Class_Bert', 'Class_Fast-Text', 'Cosine_Similarity_Bert', 'Cosine_Similarity_Fast-Text'])

df_compare_saref4inma.head(15)

# Plotting the results
x_saref4inma = df_compare_saref4inma.index.values
y1_saref4inma = df_compare_saref4inma['Cosine_Similarity_Bert']
y2_saref4inma = df_compare_saref4inma['Cosine_Similarity_Fast-Text']

plt.plot(x_saref4inma, y1_saref4inma, label='Bert Similarity')
plt.plot(x_saref4inma, y2_saref4inma, label='Fast-Text Similarity')
plt.xlabel('Index')
plt.ylabel('Cosine Similarity')
plt.title('Comparison of Cosine Similarities between Bert & Fast-Text Models in Saref4inma Ontology')
plt.legend()
plt.show()

"""## MSDL"""

# Get the list of classes with annotations
msdl_class = []

for cls in msdl.classes():
    annotations = cls.comment
    if annotations:
        msdl_class.append(cls.name)

msdl_class

msdl.classes()
msdl_cls = list(msdl.classes())

msdl.properties()
msdl_property = list(msdl.properties())

msdl_annotations = []
for ann in msdl_cls:
    print(f"\t{ann}: {ann.comment}")
    msdl_annotations.append(str(ann.comment))

# Remove the full stop from each element
msdl_annotations = [x.replace('.', '') for x in msdl_annotations]

for i in range(len(msdl_annotations)):
    msdl_annotations[i] = msdl_annotations[i] + "."

msdl_annotations

msdl_ann = ', '.join(msdl_annotations)

# define a regular expression to match non-word characters except full stops
regex = re.compile('[%s]' % re.escape(string.punctuation.replace('.', '')))

# apply the regular expression to remove non-word characters
msdl_ann = regex.sub('', msdl_ann)

msdl_ann

sentences_msdl = msdl_ann.split('. ')
valid_sentences_msdl = [s.strip() for s in sentences_msdl]

# Define a translation table to remove punctuations, symbols, and hyphens
translator_msdl = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences_msdl = [s.translate(translator_msdl).strip() for s in valid_sentences_msdl]

#Removing unnecessary punctuations from the text
valid_sentences_msdl = [n.replace('\n', '') for n in valid_sentences_msdl]
valid_sentences_msdl = [n.replace('\t', '') for n in valid_sentences_msdl]
valid_sentences_msdl = [n.replace('–', '') for n in valid_sentences_msdl]

valid_sentences_msdl = [item for item in valid_sentences_msdl if item != ' ' and item != '']

valid_sentences_msdl

# Chexking for duplicates
unique_items = set(valid_sentences_msdl)
duplicates = [item for item in unique_items if valid_sentences_msdl.count(item) > 1]
duplicates

len(valid_sentences_msdl)

"""### Bert Model"""

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Create empty lists for embeddings
bert_embeddings = []
bert_embeddings_msdl = []

# Loop over sentences and calculate embeddings
for sent1 in valid_sentences:
    embedding1 = get_embedding(sent1, model, tokenizer)
    bert_embeddings.append(embedding1)
    for sent2 in valid_sentences_msdl:
        embedding2 = get_embedding(sent2, model, tokenizer)
        bert_embeddings_msdl.append(embedding2)

# Create dataframe
data_msdl = {
    "Input_Sentence": [],
    "Annotation": [],
    "Input_Sentence_Embedding": [],
    "Annotation_Embedding": [],
}

for sent1, emb1 in zip(valid_sentences, bert_embeddings):
    for sent2, emb2 in zip(valid_sentences_msdl, bert_embeddings_msdl):
        data_msdl["Input_Sentence"].append(sent1)
        data_msdl["Annotation"].append(sent2)
        data_msdl["Input_Sentence_Embedding"].append(emb1)
        data_msdl["Annotation_Embedding"].append(emb2)

df_msdl = pd.DataFrame(data_msdl)

df_msdl.head(4)

# Adding class to the dataframe
rep_class = []
for i in range(len(valid_sentences)):
    rep_class += msdl_class

df_msdl['Class'] = rep_class[:len(df_msdl)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_msdl['Cosine_Similarity'] = df_msdl.apply(cosine_sim, axis=1)

df_msdl.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_msdl = df_msdl.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_msdl = pd.merge(df_msdl, max_similarities_msdl, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_msdl.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_msdl.reset_index(drop=True, inplace=True)

df_cosine_similarity_msdl.head(17)

# Create a new dataframe for hash codes
df_cosine_similarity_hc_msdl = df_cosine_similarity_msdl.copy()

# Removing the Embedding columns
del df_cosine_similarity_hc_msdl['Input_Sentence_Embedding']
del df_cosine_similarity_hc_msdl['Annotation_Embedding']

# create a new column with the hash code of the Input Sentence column
df_cosine_similarity_hc_msdl['Hash_Code'] = df_cosine_similarity_hc_msdl['Input_Sentence'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())

df_cosine_similarity_hc_msdl.head(3)

"""### Populating the Hash-Code within the ontology"""

# Iterate through each row of the dataframe
for index, row in df_cosine_similarity_hc_msdl.iterrows():
    # Get the hash code and the class from the current row
    hash_code = row['Hash_Code']
    class_name = row['Class']
    # Iterate through the classes in the ontology
    for cls in class_name:
        # Get the namespace of the ontology
        ns = msdl.get_namespace(base_iri=msdl.base_iri)
        # Get the class from the namespace
        cls = getattr(ns, class_name, None)
        # Check if the class was found in the ontology
        if cls is not None and isinstance(cls, ThingClass):
            # Create a new instance of the class with the hash code as the name
            instance = cls(hash_code)
        else:
            print(f"Class {class_name} not found in ontology")
        break  # exit the inner loop since we've found the matching class

# Save the ontology
msdl.save("msdl.owl")

for i in msdl.Capability.instances(): print(i)

# Print the individuals
for individual in msdl.individuals():
    print(individual)

# Get a generator object for all individuals in the ontology
individuals_msdl = msdl.individuals()

# Loop over the generator and print the URI of each individual
for individual in individuals_msdl:
    individual_uri_msdl = individual.iri
    print("Individual URI:", individual_uri_msdl)

# Iterate over all individuals in the ontology
for individual in msdl.individuals():
    print("Individual:", individual.iri)
    # Get the list of classes that the individual is a direct instance of
    classes_msdl = individual.is_a
    # Print the classes
    for cls in classes_msdl:
        print("Class:", cls.iri)

"""### Fast-Text"""

# Convert the sentences to lowercase
lowercase_sentences = [sentence.lower() for sentence in valid_sentences]
lowercase_sentences_msdl = [sentence.lower() for sentence in valid_sentences_msdl]

# create a dictionary to map the original sentences with the converted lowercase sentences for further use
dict_msdl = {}
dict_ann_msdl = {}

# loop over the lists and add the elements as key-value pairs in the dictionary
for i in range(len(lowercase_sentences)):
    dict_msdl[lowercase_sentences[i]] = valid_sentences[i]

for i in range(len(lowercase_sentences_msdl)):
    dict_ann_msdl[lowercase_sentences_msdl[i]] = valid_sentences_msdl[i]

# Split the sentences into words
lowercase_words = []
for sentence in lowercase_sentences:
    lowercase_words.extend(sentence.split())

lowercase_words_msdl = []
for sentence in lowercase_sentences_msdl:
    lowercase_words_msdl.extend(sentence.split())

# Write the input sentences to a text file
with open('ft_sentences_msdl.txt', 'w') as f:
    for s1 in lowercase_words:
        f.write(s1 + '\n')

# Write the annotations to a text file
with open('ft_sentences_ann_msdl.txt', 'w') as f:
    for s2 in lowercase_words_msdl:
        f.write(s2 + '\n')

# Create dataframe with every combination of sentences
combos = list(itertools.product(lowercase_sentences, lowercase_sentences_msdl))
df_msdl_ft = pd.DataFrame(combos, columns=["Input_Sentence", "Annotation"])

# Train the FastText model
ft_model_msdl = fasttext.train_unsupervised('ft_sentences_msdl.txt', model='skipgram', dim=100)
ft_model_ann_msdl = fasttext.train_unsupervised('ft_sentences_ann_msdl.txt', model='skipgram', dim=100)

# Function to create embeddings for the sentences
def create_embedding_msdl(sentence):
    return ft_model_msdl.get_sentence_vector(sentence)

def create_embedding_ann_msdl(sentence):
    return ft_model_ann_msdl.get_sentence_vector(sentence)

# Apply function to each row of the DataFrame
df_msdl_ft['Input_Sentence_Embedding'] = df_msdl_ft['Input_Sentence'].apply(create_embedding_msdl)
df_msdl_ft['Annotation_Embedding'] = df_msdl_ft['Annotation'].apply(create_embedding_ann_msdl)

df_msdl_ft.head(10)

# Adding class to the dataframe
rep_class_ft = []
for i in range(len(lowercase_sentences)):
    rep_class_ft += msdl_class

df_msdl_ft['Class'] = rep_class_ft[:len(df_msdl_ft)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_msdl_ft['Cosine_Similarity'] = df_msdl_ft.apply(cosine_sim, axis=1)

df_msdl_ft.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_ft_msdl = df_msdl_ft.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_ft_msdl = pd.merge(df_msdl_ft, max_similarities_ft_msdl, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_ft_msdl.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_ft_msdl.reset_index(drop=True, inplace=True)

df_cosine_similarity_ft_msdl.head(20)

# Removing the Embedding columns
del df_cosine_similarity_ft_msdl['Input_Sentence_Embedding']
del df_cosine_similarity_ft_msdl['Annotation_Embedding']

"""### Comparing Bert vs Fast-Text Models"""

# switch every row of the 'Input_Sentence' & 'Annotation' columns of the Fast-Text dataframe to the dictionary values for comparison
df_cosine_similarity_ft_msdl['Input_Sentence'] = df_cosine_similarity_ft_msdl['Input_Sentence'].map(dict_msdl)
df_cosine_similarity_ft_msdl['Annotation'] = df_cosine_similarity_ft_msdl['Annotation'].map(dict_ann_msdl)

# Creating new dataframe for comparison
df_compare_msdl = pd.merge(df_cosine_similarity_hc_msdl, df_cosine_similarity_ft_msdl, how = 'left', on = 'Input_Sentence')

# Renaming the columns
df_compare_msdl.rename(columns={'Annotation_x': 'Annotation_Bert', 'Annotation_y': 'Annotation_Fast-Text', 'Class_x': 'Class_Bert',
                                 'Class_y': 'Class_Fast-Text', 'Cosine_Similarity_x': 'Cosine_Similarity_Bert', 'Cosine_Similarity_y': 'Cosine_Similarity_Fast-Text'}, inplace=True)

# Removing unused columns
del df_compare_msdl['Hash_Code']

# Changing the positons of the columns
df_compare_msdl = df_compare_msdl.reindex(columns=['Input_Sentence', 'Annotation_Bert', 'Annotation_Fast-Text', 'Class_Bert', 'Class_Fast-Text', 'Cosine_Similarity_Bert', 'Cosine_Similarity_Fast-Text'])

df_compare_msdl.head(15)

# Plotting the results
x_msdl = df_compare_msdl.index.values
y1_msdl = df_compare_msdl['Cosine_Similarity_Bert']
y2_msdl = df_compare_msdl['Cosine_Similarity_Fast-Text']

plt.plot(x_msdl, y1_msdl, label='Bert Similarity')
plt.plot(x_msdl, y2_msdl, label='Fast-Text Similarity')
plt.xlabel('Index')
plt.ylabel('Cosine Similarity')
plt.title('Comparison of Cosine Similarities between Bert & Fast-Text Models in MSDL Ontology')
plt.legend()
plt.show()

"""## Pronto"""

# Get the list of classes with annotations
pronto_class = []

for cls in pronto.classes():
    annotations = cls.comment
    if annotations:
        pronto_class.append(cls.name)

pronto_class

pronto.classes()
pronto_cls = list(pronto.classes())

pronto.properties()
pronto_property = list(pronto.properties())

pronto_annotations = []
for ann in pronto_cls:
    print(f"\t{ann}: {ann.comment}")
    pronto_annotations.append(str(ann.comment))

# Remove the full stop from each element
pronto_annotations = [x.replace('.', '') for x in pronto_annotations]

for i in range(len(pronto_annotations)):
    pronto_annotations[i] = pronto_annotations[i] + "."

pronto_annotations

pronto_ann = ', '.join(pronto_annotations)

# define a regular expression to match non-word characters except full stops
regex = re.compile('[%s]' % re.escape(string.punctuation.replace('.', '')))

# apply the regular expression to remove non-word characters
pronto_ann = regex.sub('', pronto_ann)
pronto_ann = re.sub(r'\b(?<!\w)en\b(?!\w)', '', pronto_ann)

pronto_ann

sentences_pronto = pronto_ann.split('. ')
valid_sentences_pronto = [s.strip() for s in sentences_pronto]

# Define a translation table to remove punctuations, symbols, and hyphens
translator_pronto = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences_pronto = [s.translate(translator_pronto).strip() for s in valid_sentences_pronto]

#Removing unnecessary punctuations from the text
valid_sentences_pronto = [n.replace('\n', '') for n in valid_sentences_pronto]
valid_sentences_pronto = [n.replace('locstr', '') for n in valid_sentences_pronto]
valid_sentences_pronto = [n.replace('–', '') for n in valid_sentences_pronto]

valid_sentences_pronto = [item for item in valid_sentences_pronto if item != ' ' and item != '']

valid_sentences_pronto

# Chexking for duplicates
unique_items = set(valid_sentences_pronto)
duplicates = [item for item in unique_items if valid_sentences_pronto.count(item) > 1]
duplicates

len(valid_sentences_pronto)

"""### Bert Model"""

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Create empty lists for embeddings
bert_embeddings = []
bert_embeddings_pronto = []

# Loop over sentences and calculate embeddings
for sent1 in valid_sentences:
    embedding1 = get_embedding(sent1, model, tokenizer)
    bert_embeddings.append(embedding1)
    for sent2 in valid_sentences_pronto:
        embedding2 = get_embedding(sent2, model, tokenizer)
        bert_embeddings_pronto.append(embedding2)

# Create dataframe
data_pronto = {
    "Input_Sentence": [],
    "Annotation": [],
    "Input_Sentence_Embedding": [],
    "Annotation_Embedding": [],
}

for sent1, emb1 in zip(valid_sentences, bert_embeddings):
    for sent2, emb2 in zip(valid_sentences_pronto, bert_embeddings_pronto):
        data_pronto["Input_Sentence"].append(sent1)
        data_pronto["Annotation"].append(sent2)
        data_pronto["Input_Sentence_Embedding"].append(emb1)
        data_pronto["Annotation_Embedding"].append(emb2)

df_pronto = pd.DataFrame(data_pronto)

df_pronto.head(4)

# Adding class to the dataframe
rep_class = []
for i in range(len(valid_sentences)):
    rep_class += pronto_class

df_pronto['Class'] = rep_class[:len(df_pronto)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_pronto['Cosine_Similarity'] = df_pronto.apply(cosine_sim, axis=1)

df_pronto.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_pronto = df_pronto.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_pronto = pd.merge(df_pronto, max_similarities_pronto, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_pronto.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_pronto.reset_index(drop=True, inplace=True)

df_cosine_similarity_pronto.tail(6)

# Create a new dataframe for hash codes
df_cosine_similarity_hc_pronto = df_cosine_similarity_pronto.copy()

# Removing the Embedding columns
del df_cosine_similarity_hc_pronto['Input_Sentence_Embedding']
del df_cosine_similarity_hc_pronto['Annotation_Embedding']

# create a new column with the hash code of the Input Sentence column
df_cosine_similarity_hc_pronto['Hash_Code'] = df_cosine_similarity_hc_pronto['Input_Sentence'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())

df_cosine_similarity_hc_pronto.head(3)

"""### Populating the Hash-Code within the ontology"""

# Iterate through each row of the dataframe
for index, col in df_cosine_similarity_hc_pronto.iterrows():
    # Get the hash code and the class from the current row
    hash_code = col['Hash_Code']
    class_name = col['Class']
    # Iterate through the classes in the ontology
    for cls in class_name:
        # Get the namespace of the ontology
        ns = pronto.get_namespace(base_iri=pronto.base_iri)
        # Get the class from the namespace
        cls = getattr(ns, class_name, None)
        # Check if the class was found in the ontology
        if cls is not None and isinstance(cls, ThingClass):
            # Create a new instance of the class with the hash code as the name
            instance = cls(hash_code)
        else:
            print(f"Class {class_name} not found in ontology")
        break  # exit the inner loop since we've found the matching class

# Save the ontology
pronto.save("bpo.rdf")

for i in pronto.Product.instances(): print(i)

# Print the individuals
for individual in pronto.individuals():
    print(individual)

# Get a generator object for all individuals in the ontology
individuals = pronto.individuals()

# Loop over the generator and print the URI of each individual
for individual in individuals:
    individual_uri = individual.iri
    print("Individual URI:", individual_uri)

# Iterate over all individuals in the ontology
for individual in pronto.individuals():
    print("Individual:", individual.iri)
    # Get the list of classes that the individual is a direct instance of
    classes = individual.is_a
    # Print the classes
    for cls in classes:
        print("Class:", cls.iri)

"""### Fast-Text"""

# Convert the sentences to lowercase
lowercase_sentences = [sentence.lower() for sentence in valid_sentences]
lowercase_sentences_pronto = [sentence.lower() for sentence in valid_sentences_pronto]

# create a dictionary to map the original sentences with the converted lowercase sentences for further use
dict_pronto = {}
dict_ann_pronto = {}

# loop over the lists and add the elements as key-value pairs in the dictionary
for i in range(len(lowercase_sentences)):
    dict_pronto[lowercase_sentences[i]] = valid_sentences[i]

for i in range(len(lowercase_sentences_pronto)):
    dict_ann_pronto[lowercase_sentences_pronto[i]] = valid_sentences_pronto[i]

# Split the sentences into words
lowercase_words = []
for sentence in lowercase_sentences:
    lowercase_words.extend(sentence.split())

lowercase_words_pronto = []
for sentence in lowercase_sentences_pronto:
    lowercase_words_pronto.extend(sentence.split())

# Write the input sentences to a text file
with open('ft_sentences_pronto.txt', 'w') as f:
    for s1 in lowercase_words:
        f.write(s1 + '\n')

# Write the annotations to a text file
with open('ft_sentences_ann_pronto.txt', 'w') as f:
    for s2 in lowercase_words_pronto:
        f.write(s2 + '\n')

# Create dataframe with every combination of sentences
combos = list(itertools.product(lowercase_sentences, lowercase_sentences_pronto))
df_pronto_ft = pd.DataFrame(combos, columns=["Input_Sentence", "Annotation"])

# Train the FastText model
ft_model_pronto = fasttext.train_unsupervised('ft_sentences_pronto.txt', model='skipgram', dim=100)
ft_model_ann_pronto = fasttext.train_unsupervised('ft_sentences_ann_pronto.txt', model='skipgram', dim=100)

# Function to create embeddings for the sentences
def create_embedding_pronto(sentence):
    return ft_model_pronto.get_sentence_vector(sentence)

def create_embedding_ann_pronto(sentence):
    return ft_model_ann_pronto.get_sentence_vector(sentence)

# Apply function to each row of the DataFrame
df_pronto_ft['Input_Sentence_Embedding'] = df_pronto_ft['Input_Sentence'].apply(create_embedding_pronto)
df_pronto_ft['Annotation_Embedding'] = df_pronto_ft['Annotation'].apply(create_embedding_ann_pronto)

df_pronto_ft.head(10)

# Adding class to the dataframe
rep_class_ft = []
for i in range(len(lowercase_sentences)):
    rep_class_ft += pronto_class

df_pronto_ft['Class'] = rep_class_ft[:len(df_pronto_ft)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_pronto_ft['Cosine_Similarity'] = df_pronto_ft.apply(cosine_sim, axis=1)

df_pronto_ft.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_ft_pronto = df_pronto_ft.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_ft_pronto = pd.merge(df_pronto_ft, max_similarities_ft_pronto, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_ft_pronto.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_ft_pronto.reset_index(drop=True, inplace=True)

df_cosine_similarity_ft_pronto.head(20)

# Removing the Embedding columns
del df_cosine_similarity_ft_pronto['Input_Sentence_Embedding']
del df_cosine_similarity_ft_pronto['Annotation_Embedding']

"""### Comparing Bert vs Fast-Text Models"""

# Switching every row of the 'Input_Sentence' & 'Annotation' columns of the Fast-Text dataframe to the dictionary values for comparison
df_cosine_similarity_ft_pronto['Input_Sentence'] = df_cosine_similarity_ft_pronto['Input_Sentence'].map(dict_pronto)
df_cosine_similarity_ft_pronto['Annotation'] = df_cosine_similarity_ft_pronto['Annotation'].map(dict_ann_pronto)

# Creating new dataframe for comparison
df_compare_pronto = pd.merge(df_cosine_similarity_hc_pronto, df_cosine_similarity_ft_pronto, how = 'left', on = 'Input_Sentence')

# Renaming the columns
df_compare_pronto.rename(columns={'Annotation_x': 'Annotation_Bert', 'Annotation_y': 'Annotation_Fast-Text', 'Class_x': 'Class_Bert',
                                 'Class_y': 'Class_Fast-Text', 'Cosine_Similarity_x': 'Cosine_Similarity_Bert', 'Cosine_Similarity_y': 'Cosine_Similarity_Fast-Text'}, inplace=True)

# Removing the unused columns
del df_compare_pronto['Hash_Code']

# Changing the positons of the columns
df_compare_pronto = df_compare_pronto.reindex(columns=['Input_Sentence', 'Annotation_Bert', 'Annotation_Fast-Text', 'Class_Bert', 'Class_Fast-Text', 'Cosine_Similarity_Bert', 'Cosine_Similarity_Fast-Text'])

df_compare_pronto.head(15)

# Plotting the results
x_pronto = df_compare_pronto.index.values
y1_pronto = df_compare_pronto['Cosine_Similarity_Bert']
y2_pronto = df_compare_pronto['Cosine_Similarity_Fast-Text']

plt.plot(x_pronto, y1_pronto, label='Bert Similarity')
plt.plot(x_pronto, y2_pronto, label='Fast-Text Similarity')
plt.xlabel('Index')
plt.ylabel('Cosine Similarity')
plt.title('Comparison of Cosine Similarities between Bert & Fast-Text Models in Pronto Ontology')
plt.legend()
plt.show()

"""## SMO"""

# Get the list of classes with annotations
smo_class = []

for cls in smo.classes():
    annotations = cls.comment
    if annotations:
        smo_class.append(cls.name)

smo_class

smo.classes()
smo_cls = list(smo.classes())

smo.properties()
smo_property = list(smo.properties())

smo_annotations = []
for ann in smo_cls:
    print(f"\t{ann}: {ann.comment}")
    smo_annotations.append(str(ann.comment))

# Remove the full stop from each element
smo_annotations = [x.replace('.', '') for x in smo_annotations]

for i in range(len(smo_annotations)):
    smo_annotations[i] = smo_annotations[i] + "."

smo_annotations

smo_ann = ', '.join(smo_annotations)

# define a regular expression to match non-word characters except full stops
regex = re.compile('[%s]' % re.escape(string.punctuation.replace('.', '')))

# apply the regular expression to remove non-word characters
smo_ann = regex.sub('', smo_ann)
smo_ann = re.sub(r'\b(?<!\w)en\b(?!\w)', '', smo_ann)

smo_ann

sentences_smo = smo_ann.split('. ')
valid_sentences_smo = [s.strip() for s in sentences_smo]

# Define a translation table to remove punctuations, symbols, and hyphens
translator_smo = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences_smo = [s.translate(translator_smo).strip() for s in valid_sentences_smo]

#Removing unnecessary punctuations from the text
valid_sentences_smo = [n.replace('\n', '') for n in valid_sentences_smo]
valid_sentences_smo = [n.replace('locstr', '') for n in valid_sentences_smo]
valid_sentences_smo = [n.replace('–', '') for n in valid_sentences_smo]

valid_sentences_smo = [item for item in valid_sentences_smo if item != ' ' and item != '']

valid_sentences_smo

# Chexking for duplicates
unique_items = set(valid_sentences_smo)
duplicates = [item for item in unique_items if valid_sentences_smo.count(item) > 1]
duplicates

len(valid_sentences_smo)

"""### Bert Model"""

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Create empty lists for embeddings
bert_embeddings = []
bert_embeddings_smo = []

# Loop over sentences and calculate embeddings
for sent1 in valid_sentences:
    embedding1 = get_embedding(sent1, model, tokenizer)
    bert_embeddings.append(embedding1)
    for sent2 in valid_sentences_smo:
        embedding2 = get_embedding(sent2, model, tokenizer)
        bert_embeddings_smo.append(embedding2)

# Create dataframe
data_smo = {
    "Input_Sentence": [],
    "Annotation": [],
    "Input_Sentence_Embedding": [],
    "Annotation_Embedding": [],
}

for sent1, emb1 in zip(valid_sentences, bert_embeddings):
    for sent2, emb2 in zip(valid_sentences_smo, bert_embeddings_smo):
        data_smo["Input_Sentence"].append(sent1)
        data_smo["Annotation"].append(sent2)
        data_smo["Input_Sentence_Embedding"].append(emb1)
        data_smo["Annotation_Embedding"].append(emb2)

df_smo = pd.DataFrame(data_smo)

df_smo.head(4)

# Adding class to the dataframe
rep_class = []
for i in range(len(valid_sentences)):
    rep_class += smo_class

df_smo['Class'] = rep_class[:len(df_smo)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_smo['Cosine_Similarity'] = df_smo.apply(cosine_sim, axis=1)

df_smo.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_smo = df_smo.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_smo = pd.merge(df_smo, max_similarities_smo, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_smo.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_smo.reset_index(drop=True, inplace=True)

df_cosine_similarity_smo.head(17)

# Create a new dataframe for hash codes
df_cosine_similarity_hc_smo = df_cosine_similarity_smo.copy()

# Removing the Embedding columns
del df_cosine_similarity_hc_smo['Input_Sentence_Embedding']
del df_cosine_similarity_hc_smo['Annotation_Embedding']

# create a new column with the hash code of the Input Sentence column
df_cosine_similarity_hc_smo['Hash_Code'] = df_cosine_similarity_hc_smo['Input_Sentence'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())

df_cosine_similarity_hc_smo.head(3)

"""### Populating the Hash-Code within the ontology"""

# Iterate through each row of the dataframe
for index, col in df_cosine_similarity_hc_smo.iterrows():
    # Get the hash code and the class from the current row
    hash_code = col['Hash_Code']
    class_name = col['Class']
    # Iterate through the classes in the ontology
    for cls in class_name:
        # Get the namespace of the ontology
        ns = smo.get_namespace(base_iri=smo.base_iri)
        # Get the class from the namespace
        cls = getattr(ns, class_name, None)
        # Check if the class was found in the ontology
        if cls is not None and isinstance(cls, ThingClass):
            # Create a new instance of the class with the hash code as the name
            instance = cls(hash_code)
        else:
            print(f"Class {class_name} not found in ontology")
        break  # exit the inner loop since we've found the matching class

# Save the ontology
smo.save("smo.owl")

for i in smo.Feeder.instances(): print(i)

# Print the individuals
for individual in smo.individuals():
    print(individual)

# Get a generator object for all individuals in the ontology
individuals = smo.individuals()

# Loop over the generator and print the URI of each individual
for individual in individuals:
    individual_uri = individual.iri
    print("Individual URI:", individual_uri)

# Iterate over all individuals in the ontology
for individual in smo.individuals():
    print("Individual:", individual.iri)
    # Get the list of classes that the individual is a direct instance of
    classes = individual.is_a
    # Print the classes
    for cls in classes:
        print("Class:", cls.iri)

"""### Fast-Text"""

# Convert the sentences to lowercase
lowercase_sentences = [sentence.lower() for sentence in valid_sentences]
lowercase_sentences_smo = [sentence.lower() for sentence in valid_sentences_smo]

# create a dictionary to map the original sentences with the converted lowercase sentences for further use
dict_smo = {}
dict_ann_smo = {}

# loop over the lists and add the elements as key-value pairs in the dictionary
for i in range(len(lowercase_sentences)):
    dict_smo[lowercase_sentences[i]] = valid_sentences[i]

for i in range(len(lowercase_sentences_smo)):
    dict_ann_smo[lowercase_sentences_smo[i]] = valid_sentences_smo[i]

# Split the sentences into words
lowercase_words = []
for sentence in lowercase_sentences:
    lowercase_words.extend(sentence.split())

lowercase_words_smo = []
for sentence in lowercase_sentences_smo:
    lowercase_words_smo.extend(sentence.split())

# Write the input sentences to a text file
with open('ft_sentences_smo.txt', 'w') as f:
    for s1 in lowercase_words:
        f.write(s1 + '\n')

# Write the annotations to a text file
with open('ft_sentences_ann_smo.txt', 'w') as f:
    for s2 in lowercase_words_smo:
        f.write(s2 + '\n')

# Create dataframe with every combination of sentences
combos = list(itertools.product(lowercase_sentences, lowercase_sentences_smo))
df_smo_ft = pd.DataFrame(combos, columns=["Input_Sentence", "Annotation"])

# Train the FastText model
ft_model_smo = fasttext.train_unsupervised('ft_sentences_smo.txt', model='skipgram', dim=100)
ft_model_ann_smo = fasttext.train_unsupervised('ft_sentences_ann_smo.txt', model='skipgram', dim=100)

# Function to create embeddings for the sentences
def create_embedding_smo(sentence):
    return ft_model_smo.get_sentence_vector(sentence)

def create_embedding_ann_smo(sentence):
    return ft_model_ann_smo.get_sentence_vector(sentence)

# Apply function to each row of the DataFrame
df_smo_ft['Input_Sentence_Embedding'] = df_smo_ft['Input_Sentence'].apply(create_embedding_smo)
df_smo_ft['Annotation_Embedding'] = df_smo_ft['Annotation'].apply(create_embedding_ann_smo)

df_smo_ft.head(10)

# Adding class to the dataframe
rep_class_ft = []
for i in range(len(lowercase_sentences)):
    rep_class_ft += smo_class

df_smo_ft['Class'] = rep_class_ft[:len(df_smo_ft)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_smo_ft['Cosine_Similarity'] = df_smo_ft.apply(cosine_sim, axis=1)

df_smo_ft.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_ft_smo = df_smo_ft.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_ft_smo = pd.merge(df_smo_ft, max_similarities_ft_smo, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_ft_smo.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_ft_smo.reset_index(drop=True, inplace=True)

df_cosine_similarity_ft_smo.head(20)

# Removing the Embedding columns
del df_cosine_similarity_ft_smo['Input_Sentence_Embedding']
del df_cosine_similarity_ft_smo['Annotation_Embedding']

"""### Comparing Bert vs Fast-Text Models"""

# Switching every row of the 'Input_Sentence' & 'Annotation' columns of the Fast-Text dataframe to the dictionary values for comparison
df_cosine_similarity_ft_smo['Input_Sentence'] = df_cosine_similarity_ft_smo['Input_Sentence'].map(dict_smo)
df_cosine_similarity_ft_smo['Annotation'] = df_cosine_similarity_ft_smo['Annotation'].map(dict_ann_smo)

# Creating new dataframe for comparison
df_compare_smo = pd.merge(df_cosine_similarity_hc_smo, df_cosine_similarity_ft_smo, how = 'left', on = 'Input_Sentence')

# Renaming the columns
df_compare_smo.rename(columns={'Annotation_x': 'Annotation_Bert', 'Annotation_y': 'Annotation_Fast-Text', 'Class_x': 'Class_Bert',
                                 'Class_y': 'Class_Fast-Text', 'Cosine_Similarity_x': 'Cosine_Similarity_Bert', 'Cosine_Similarity_y': 'Cosine_Similarity_Fast-Text'}, inplace=True)

# Removing the unused columns
del df_compare_smo['Hash_Code']

# Changing the positons of the columns
df_compare_smo = df_compare_smo.reindex(columns=['Input_Sentence', 'Annotation_Bert', 'Annotation_Fast-Text', 'Class_Bert', 'Class_Fast-Text', 'Cosine_Similarity_Bert', 'Cosine_Similarity_Fast-Text'])

df_compare_smo.head(15)

# Plotting the results
x_smo = df_compare_smo.index.values
y1_smo = df_compare_smo['Cosine_Similarity_Bert']
y2_smo = df_compare_smo['Cosine_Similarity_Fast-Text']

plt.plot(x_smo, y1_smo, label='Bert Similarity')
plt.plot(x_smo, y2_smo, label='Fast-Text Similarity')
plt.xlabel('Index')
plt.ylabel('Cosine Similarity')
plt.title('Comparison of Cosine Similarities between Bert & Fast-Text Models in Smart Manufacturing Ontology')
plt.legend()
plt.show()

"""## Choosing the best ontology with Cosine Similarity"""

# Add the ontology column
df_cosine_similarity_mason['Ontology'] = 'Mason'
df_cosine_similarity_saref4inma['Ontology'] = 'Saref'
df_cosine_similarity_msdl['Ontology'] = 'MSDL'
df_cosine_similarity_pronto['Ontology'] = 'Pronto'
df_cosine_similarity_smo['Ontology'] = 'SMO'

# Removing the Embedding columns from all dataframes
df_cosine_similarity_mason = df_cosine_similarity_mason.drop(['Input_Sentence_Embedding', 'Annotation_Embedding'], axis=1)
df_cosine_similarity_saref4inma = df_cosine_similarity_saref4inma.drop(['Input_Sentence_Embedding', 'Annotation_Embedding'], axis=1)
df_cosine_similarity_msdl = df_cosine_similarity_msdl.drop(['Input_Sentence_Embedding', 'Annotation_Embedding'], axis=1)
df_cosine_similarity_pronto = df_cosine_similarity_pronto.drop(['Input_Sentence_Embedding', 'Annotation_Embedding'], axis=1)
df_cosine_similarity_smo = df_cosine_similarity_smo.drop(['Input_Sentence_Embedding', 'Annotation_Embedding'], axis=1)

df_onto_merge = pd.concat([df_cosine_similarity_mason, df_cosine_similarity_saref4inma, df_cosine_similarity_msdl, df_cosine_similarity_pronto, df_cosine_similarity_smo], ignore_index=True)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_onto = df_onto_merge.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_onto = pd.merge(df_onto_merge, max_similarities_onto, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_onto.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_onto.reset_index(drop=True, inplace=True)

df_onto.head(30)

"""# MLP

## Annotations
"""

# Combine all the sannotations and their respective ontology names into a single list
ann_list = valid_sentences_mason + valid_sentences_saref4inma + valid_sentences_msdl + valid_sentences_pronto + valid_sentences_smo
ann_onto_names = ['Mason'] * len(valid_sentences_mason) + ['Saref'] * len(valid_sentences_saref4inma) + ['MSDL'] * len(valid_sentences_msdl) + ['Pronto'] * len(valid_sentences_pronto) + ['SMO'] * len(valid_sentences_smo)

# Create a dataframe with the annotations and their ontology names
df_mlp = pd.DataFrame({'Annotation': ann_list, 'Ontology': ann_onto_names})

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Apply the Bert model to obtain annotation embeddings
df_mlp["Annotation_Embedding"] = df_mlp["Annotation"].apply(lambda x: get_embedding(x, model, tokenizer))

df_mlp.head(10)

X = df_mlp["Annotation_Embedding"].tolist()
y = df_mlp["Ontology"].tolist()

# Encode labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

input_dim = 768  # Size of BERT embeddings
hidden_dim = 768  # Size of hidden layer
output_dim = len(label_encoder.classes_)  # Number of unique labels

model_mlp = MLP(input_dim, hidden_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_mlp.parameters(), lr=0.001)

X_train = torch.tensor(X_train)
y_train = torch.tensor(y_train)
X_test = torch.tensor(X_test)
y_test = torch.tensor(y_test)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

num_epochs = 10
batch_size = 32

for epoch in range(num_epochs):
    running_loss = 0.0
    for i in range(0, len(X_train), batch_size):
        inputs = X_train[i:i+batch_size]
        labels = y_train[i:i+batch_size]

        optimizer.zero_grad()

        outputs = model_mlp(inputs.float())
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Calculate validation loss
    with torch.no_grad():
        val_outputs = model_mlp(X_val.float())
        val_loss = criterion(val_outputs, y_val)

    print(f'Epoch {epoch+1} - Training Loss: {running_loss/len(X_train)} - Validation Loss: {val_loss.item()}')

with torch.no_grad():
    test_outputs = model_mlp(X_test.float())
    predicted = torch.argmax(test_outputs, dim=1)
    accuracy = (predicted == y_test).sum().item() / len(y_test)
    print(f'Test Accuracy: {accuracy}')

# Convert the predicted numpy array to a PyTorch tensor
predicted = predicted.numpy()

# Compute the confusion matrix
cm = confusion_matrix(y_test, predicted)

# Create a heatmap plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Convert the predicted tensor to a numpy array
predicted = torch.from_numpy(predicted)

# Calculate precision, recall, and F1 score
precision = precision_score(y_test, predicted, average='macro')
recall = recall_score(y_test, predicted, average='macro')
f1 = f1_score(y_test, predicted, average='macro')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""## Projection of Annotation Embeddings"""

# Extract embeddings and ontology names
embeddings_ann = np.array(df_mlp["Annotation_Embedding"].tolist())
ontology_names = df_mlp["Ontology"].tolist()

# Perform t-SNE dimensionality reduction
tsne = TSNE(n_components=2, random_state=42)
embeddings_proj = tsne.fit_transform(embeddings_ann)

# Create a new DataFrame with the embeddings and ontology names
df_projection = pd.DataFrame(embeddings_proj, columns=['X', 'Y'])
df_projection["Ontology"] = ontology_names

# Plot the projection
plt.figure(figsize=(10, 8))
for ontology_name in df_projection["Ontology"].unique():
    subset = df_projection[df_projection["Ontology"] == ontology_name]
    plt.scatter(subset['X'], subset['Y'], label=ontology_name)

plt.title('Projection of Annotation Embeddings of different Ontologies')
plt.legend()
plt.show()

"""## Choosing the best ontology with MLP

### Input Text
"""

#Create a dataframe with the input sentences
df_input_mlp = pd.DataFrame({'Input_Sentence': valid_sentences})

# Apply the Bert model to obtain input sentences embeddings
df_input_mlp["Input_Sentence_Embedding"] = df_input_mlp["Input_Sentence"].apply(lambda x: get_embedding(x, model, tokenizer))

df_input_mlp.head(17)

# Convert BERT embeddings to tensors
X_new = torch.tensor(df_input_mlp['Input_Sentence_Embedding'].tolist())

with torch.no_grad():
    outputs = model_mlp(X_new.float())
    predicted_labels = label_encoder.inverse_transform(torch.argmax(outputs, dim=1))

df_input_mlp['Predicted Ontology'] = predicted_labels

df_input_mlp = df_input_mlp.drop(['Input_Sentence_Embedding'], axis=1)

df_input_mlp.head(30)

"""# Chosen Ontology by both the Methods"""

# Merge both the dataframes
df_chosen_onto = pd.merge(df_onto, df_input_mlp, on='Input_Sentence', how='left')

# Removing unecessary columns
df_chosen_onto = df_chosen_onto.drop(['Annotation', 'Class', 'Cosine_Similarity'], axis=1)

# Renaming the columns
df_chosen_onto.rename(columns={'Ontology': 'Ontology_Cosine_Similarity', 'Predicted Ontology': 'Ontology_MLP'}, inplace=True)

df_chosen_onto.head(30)